{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import nntplib as nn\n",
    "from typing import Deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from parameters_global import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobalParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Code definiert verschiedene Konstanten und Parameter, die in einem Handelsalgorithmus verwendet werden, der auf einem verstärkten Lernen basiert. \n",
    "\n",
    "1. STATE_SPACE: Dies ist die Größe des Zustandsraums, der die Merkmale enthält, die der Agent betrachtet, um seine Entscheidungen zu treffen. In diesem Fall beträgt der Zustandsraum 34.\n",
    "\n",
    "2. ACTION_SPACE: Dies ist die Größe des Aktionsraums, der die möglichen Aktionen enthält, die der Agent ausführen kann. Hier hat der Aktionsraum eine Größe von 3, da der Agent zwischen den Aktionen \"Verkauf\" (-1), \"Nichtstun\" (0) und \"Kauf\" (1) wählen kann.\n",
    "\n",
    "3. ACTION_LOW und ACTION_HIGH: Dies sind die Grenzen des Aktionsraums, die den niedrigsten und höchsten Wert der Aktionen angeben, die der Agent ausführen kann.\n",
    "\n",
    "4. GAMMA: Dies ist der Diskontierungsfaktor, der angibt, wie stark der Agent zukünftige Belohnungen gewichtet. Ein hoher GAMMA-Wert (nahe 1) bedeutet, dass der Agent langfristige Belohnungen stärker berücksichtigt.\n",
    "\n",
    "5. TAU: Dies ist der Wert für die Soft-Update-Methode, mit der die Zielnetzwerkparameter allmählich an die aktuellen Netzwerkparameter angepasst werden, um eine stabilere Lernleistung zu erzielen.\n",
    "\n",
    "6. EPS_START, EPS_END und EPS_DECAY: Diese Parameter steuern die Exploration des Agenten während des Lernprozesses. Der Agent verwendet eine epsilon-greedy Policy, um zwischen Exploration (zufällige Aktionen) und Exploitation (Aktionen basierend auf dem bisher Gelernten) zu wählen. EPS_START gibt den Startwert für die Wahrscheinlichkeit der Exploration an, während EPS_END den Endwert angibt. EPS_DECAY bestimmt die Geschwindigkeit, mit der die Exploration im Laufe der Zeit reduziert wird.\n",
    "\n",
    "7. MEMORY_LEN: Dies ist die maximale Größe des Erfahrungsspeichers, in dem der Agent vergangene Erfahrungen speichert, um sie für das Training zu verwenden.\n",
    "\n",
    "8. MEMORY_THRESH: Die Anzahl der Erfahrungen, die im Erfahrungsspeicher gesammelt werden müssen, bevor das Training des Agenten beginnt.\n",
    "\n",
    "9. BATCH_SIZE: Die Größe der Stichprobe, die aus dem Erfahrungsspeicher für jedes Training gezogen wird.\n",
    "\n",
    "10. LR_DQN: Die Lernrate des Deep-Q-Networks, die die Geschwindigkeit des Lernprozesses beeinflusst.\n",
    "\n",
    "11. LEARN_AFTER: Die Anzahl der Erfahrungen, die im Erfahrungsspeicher gesammelt werden müssen, bevor der Agent mit dem Training beginnt.\n",
    "\n",
    "12. LEARN_EVERY: Die Häufigkeit, mit der der Agent trainiert wird, nachdem er das erste Mal gelernt hat.\n",
    "\n",
    "13. UPDATE_EVERY: Die Häufigkeit, mit der die Zielnetzwerkparameter aktualisiert werden.\n",
    "\n",
    "14. COST: Die Handelskosten, die bei jeder Aktion berücksichtigt werden.\n",
    "\n",
    "15. CAPITAL: Der Anfangsbetrag des Kapitals, mit dem der Agent handelt.\n",
    "\n",
    "16. NEG_MUL: Ein Multiplikator, der verwendet wird, um den Agenten risikoscheuer gegenüber einer negativen Rendite zu machen. Ein höherer Wert bedeutet, dass der Agent stärker bestraft wird, wenn er Verluste erleidet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = 34  #geändert anstatt 28\n",
    "ACTION_SPACE = 3\n",
    "\n",
    "ACTION_LOW = -1\n",
    "ACTION_HIGH = 1\n",
    "\n",
    "GAMMA = 0.9995\n",
    "TAU = 1e-3\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 0.9\n",
    "\n",
    "MEMORY_LEN = 10000\n",
    "MEMORY_THRESH = 500\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "LR_DQN = 5e-4\n",
    "\n",
    "LEARN_AFTER = MEMORY_THRESH\n",
    "LEARN_EVERY = 3\n",
    "UPDATE_EVERY = 9\n",
    "\n",
    "COST = 3e-4\n",
    "CAPITAL = 100000\n",
    "NEG_MUL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse \"DataGetter\" dient dazu, historische Daten für bestimmte Vermögenswerte zu sammeln und sie für einen Handelsalgorithmus vorzubereiten. Sie kann Daten für einen bestimmten Vermögenswert über einen angegebenen Zeitraum abrufen und berechnet verschiedene Merkmale wie Renditen, Handelsvolumenänderungen, Volatilität (Bollinger-Bands), Moving Average Convergence Divergence (MACD) und den Relative Strength Indicator (RSI). Diese Merkmale sind entscheidend für den Handelsalgorithmus, um fundierte Entscheidungen zu treffen. Die Methode \"scaleData\" normalisiert die Daten, um sicherzustellen, dass alle Merkmale auf einer ähnlichen Skala liegen. Durch die Nutzung dieser Klasse kann der Handelsalgorithmus auf gut vorbereitete historische Daten zugreifen und somit optimale Handelsentscheidungen treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "  \"\"\"\n",
    "  Klasse um Daten zu Erstellen\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, asset=\"BTC-USD\", start_date=None, end_date=None, freq=\"1d\", \n",
    "               timeframes=[1, 2, 5, 10, 20, 40]):\n",
    "    self.asset = asset\n",
    "    self.sd = start_date\n",
    "    self.ed = end_date\n",
    "    self.freq = freq\n",
    "\n",
    "    self.timeframes = timeframes\n",
    "    self.getData()\n",
    "\n",
    "    self.scaler = StandardScaler()\n",
    "    self.scaler.fit(self.data[:, 1:])\n",
    "\n",
    "\n",
    "  def getData(self):\n",
    "    \n",
    "    asset = self.asset  \n",
    "    if self.sd is not None and self.ed is not None:\n",
    "      df =  yf.download([asset], start=self.sd, end=self.ed, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], start=self.sd, end=self.ed, interval=self.freq)\n",
    "    elif self.sd is None and self.ed is not None:\n",
    "      df =  yf.download([asset], end=self.ed, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], end=self.ed, interval=self.freq)\n",
    "    elif self.sd is not None and self.ed is None:\n",
    "      df =  yf.download([asset], start=self.sd, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], start=self.sd, interval=self.freq)\n",
    "    else:\n",
    "      df = yf.download([asset], period=\"max\", interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], interval=self.freq)\n",
    "    \n",
    "    df[\"rf\"] = df[\"Adj Close\"].pct_change().shift(-1)\n",
    "\n",
    "    for i in self.timeframes:\n",
    "      df_spy[f\"spy_ret-{i}\"] = df_spy[\"Adj Close\"].pct_change(i)\n",
    "      df_spy[f\"spy_v-{i}\"] = df_spy[\"Volume\"].pct_change(i)\n",
    "\n",
    "      df[f\"r-{i}\"] = df[\"Adj Close\"].pct_change(i)      \n",
    "      df[f\"v-{i}\"] = df[\"Volume\"].pct_change(i)\n",
    "    \n",
    "    for i in [5, 10, 20, 40]:\n",
    "      df[f'sig-{i}'] = np.log(1 + df[\"r-1\"]).rolling(i).std()\n",
    "\n",
    "    df[\"macd_lmw\"] = df[\"r-1\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd_smw\"] = df[\"r-1\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"macd_bl\"] = df[\"r-1\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"macd\"] = df[\"macd_smw\"] - df[\"macd_lmw\"]\n",
    "\n",
    "    rsi_lb = 5\n",
    "    pos_gain = df[\"r-1\"].where(df[\"r-1\"] > 0, 0).ewm(rsi_lb).mean()\n",
    "    neg_gain = df[\"r-1\"].where(df[\"r-1\"] < 0, 0).ewm(rsi_lb).mean()\n",
    "    rs = np.abs(pos_gain/neg_gain)\n",
    "    df[\"rsi\"] = 100 * rs/(1 + rs)\n",
    "\n",
    "    bollinger_lback = 10\n",
    "    df[\"bollinger\"] = df[\"r-1\"].ewm(bollinger_lback).mean()\n",
    "    df[\"low_bollinger\"] = df[\"bollinger\"] - 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "    df[\"high_bollinger\"] = df[\"bollinger\"] + 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "    # print(df.columns)\n",
    "    # print(df_spy.columns)\n",
    "    # SP500\n",
    "    # df = df.merge(df_spy[[f\"spy_ret-{i}\" for i in self.timeframes] + [f\"spy_sig-{i}\" for i in [5, 10, 20, 40]]], \n",
    "    #               how=\"left\", right_index=True, left_index=True)\n",
    "    df = df.merge(df_spy[[f\"spy_ret-{i}\" for i in self.timeframes]], \n",
    "              how=\"left\", right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "    for c in df.columns:\n",
    "      df[c].interpolate('linear', limit_direction='both', inplace=True)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    self.frame = df\n",
    "    self.data = np.array(df.iloc[:, 6:])\n",
    "    return\n",
    "\n",
    "\n",
    "  def scaleData(self):\n",
    "    self.scaled_data = self.scaler.fit_transform(self.data[:, 1:])\n",
    "    return\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx, col_idx=None):\n",
    "    if col_idx is None:\n",
    "      return self.data[idx]\n",
    "    elif col_idx < len(list(self.data.columns)):\n",
    "      return self.data[idx][col_idx]\n",
    "    else:\n",
    "      raise IndexError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der gegebene Code enthält die Implementierung einer Replay Memory-Klasse für einen Agenten im Verstärkungslernen. Diese Klasse ermöglicht es dem Agenten, frühere Erfahrungen in Form von Zustandsübergängen zu speichern und für das Training zu nutzen. Ein Zustandsübergang besteht aus den Komponenten \"Zustände\", \"Aktionen\", \"Belohnungen\", \"Folgezustände\" und \"Abschlüsse\". Die Replay Memory-Klasse verwendet ein Deque als internen Speicher mit einer festen Kapazität, um die letzten Erfahrungen beizubehalten und ältere Erfahrungen zu verwerfen, wenn die Kapazitätsgrenze erreicht ist. Die Methode \"sample\" zieht zufällig eine Stichprobe von Erfahrungen aus dem Speicher, um das Lernen zu diversifizieren. Insgesamt ermöglicht die Replay Memory-Klasse dem Agenten, aus vergangenen Erfahrungen zu lernen und seine Entscheidungsfindung zu verbessern, um eine optimale Handelsstrategie zu entwickeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"States\", \"Actions\", \"Rewards\", \"NextStates\", \"Dones\"])\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "  \"\"\"\n",
    "  Implementierung des Agenten\n",
    "  \"\"\"\n",
    "  def __init__(self, capacity=MEMORY_LEN):\n",
    "    self.memory = Deque(maxlen=capacity)\n",
    "\n",
    "  def store(self, t):\n",
    "    self.memory.append(t)\n",
    "\n",
    "  def sample(self, n):\n",
    "    a = random.sample(self.memory, n)\n",
    "    return a\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der gegebene Code implementiert ein Duelling Deep Q Network (DQN) für einen Handelsagenten im Verstärkungslernen. Die Architektur des neuronalen Netzwerks ist in der Klasse `DuellingDQN` definiert, während die Handelsentscheidungen und das Training durch die Klasse `DQNAgent` erfolgen.\n",
    "\n",
    "Die `DuellingDQN`-Klasse definiert die Struktur des neuronalen Netzwerks mit mehreren vollständig verbundenen Schichten. Es verwendet ReLU-, Tanh- und Sigmoid-Aktivierungsfunktionen und eine Softmax-Aktivierungsfunktion in der letzten Schicht, um Aktionen vorherzusagen.\n",
    "\n",
    "Der `DQNAgent` verwendet zwei Instanzen des DuellingDQN-Netzwerks: `actor_online` für die Entscheidungsfindung und `actor_target` für die zielgerichtete Aktualisierung. Die Erfahrungen des Agenten werden in einer Replay Memory-Klasse (`memory`) gespeichert und verwendet, um das Netzwerk zu trainieren. Der Agent verwendet die Bellman-Gleichung und den Adam-Optimizer, um den Verlust des neuronalen Netzwerks zu berechnen und zu minimieren. Zusätzlich wird das Target-Netzwerk durch weiche Aktualisierung aktualisiert, um das Lernen zu stabilisieren.\n",
    "\n",
    "Der Agent kann basierend auf einem Zustand eine Aktion auswählen. Er kann entweder die Aktion mit dem höchsten Q-Wert wählen oder eine zufällige Aktion auswählen, um die Exploration und Ausbeutung auszubalancieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuellingDQN(nn.Module):\n",
    "  \"\"\"\n",
    "  Architektur\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_dim=STATE_SPACE, output_dim=ACTION_SPACE):\n",
    "    super(DuellingDQN, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "    self.fc1 = nn.Linear(self.input_dim, 500)\n",
    "    self.fc2 = nn.Linear(500, 500)\n",
    "    self.fc3 = nn.Linear(500, 300)\n",
    "    self.fc4 = nn.Linear(300, 200)\n",
    "    self.fc5 = nn.Linear(200, 10)\n",
    "\n",
    "    self.fcs = nn.Linear(10, 1)\n",
    "    self.fcp = nn.Linear(10, self.output_dim)\n",
    "    self.fco = nn.Linear(self.output_dim + 1, self.output_dim)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.sig = nn.Sigmoid()\n",
    "    self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc1(state))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    x = self.relu(self.fc3(x))\n",
    "    x = self.relu(self.fc4(x))\n",
    "    x = self.relu(self.fc5(x))\n",
    "    xs = self.relu(self.fcs(x))\n",
    "    xp = self.relu(self.fcp(x))\n",
    "\n",
    "    x = xs + xp - xp.mean()\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "  \"\"\"\n",
    "  Implementierung\n",
    "  \"\"\"\n",
    "  DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "  def __init__(self, actor_net=DuellingDQN, memory=ReplayMemory()):\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.actor_online = actor_net(STATE_SPACE, ACTION_SPACE).to(DEVICE)\n",
    "    self.actor_target = actor_net(STATE_SPACE, ACTION_SPACE).to(DEVICE)\n",
    "    self.actor_target.load_state_dict(self.actor_online.state_dict())\n",
    "    self.actor_target.eval()\n",
    "\n",
    "    self.memory = memory\n",
    "\n",
    "    self.actor_criterion = nn.MSELoss()\n",
    "    self.actor_op = optim.Adam(self.actor_online.parameters(), lr=LR_DQN)\n",
    "\n",
    "    self.t_step = 0\n",
    "\n",
    "\n",
    "  def act(self, state, eps=0.):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    self.t_step += 1\n",
    "    state = torch.from_numpy(state).float().to(DEVICE).view(1, -1)\n",
    "    \n",
    "    self.actor_online.eval()\n",
    "    with torch.no_grad():\n",
    "      actions = self.actor_online(state)\n",
    "    self.actor_online.train()\n",
    "\n",
    "    if random.random() > eps:\n",
    "      act = np.argmax(actions.cpu().data.numpy())\n",
    "    else:\n",
    "      act = random.choice(np.arange(ACTION_SPACE))\n",
    "    return int(act)\n",
    "\n",
    "\n",
    "  def learn(self):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if len(self.memory) <= MEMORY_THRESH:\n",
    "      return 0\n",
    "\n",
    "    if self.t_step > LEARN_AFTER and self.t_step % LEARN_EVERY==0:\n",
    "      batch = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "      states = np.vstack([t.States for t in batch])\n",
    "      states = torch.from_numpy(states).float().to(DEVICE)\n",
    "\n",
    "      actions = np.vstack([t.Actions for t in batch])\n",
    "      actions = torch.from_numpy(actions).float().to(DEVICE)\n",
    "\n",
    "      rewards = np.vstack([t.Rewards for t in batch])\n",
    "      rewards = torch.from_numpy(rewards).float().to(DEVICE)\n",
    "\n",
    "      next_states = np.vstack([t.NextStates for t in batch])\n",
    "      next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "\n",
    "      dones = np.vstack([t.Dones for t in batch]).astype(np.uint8)\n",
    "      dones = torch.from_numpy(dones).float().to(DEVICE)\n",
    "\n",
    "      # Update\n",
    "      # Berechnung nächster Schritte\n",
    "      next_state_values = self.actor_target(next_states).max(1)[0].unsqueeze(1)\n",
    "      y = rewards + (1-dones) * GAMMA * next_state_values\n",
    "      state_values = self.actor_online(states).gather(1, actions.type(torch.int64))\n",
    "      # Berechne Verlust\n",
    "      actor_loss = self.actor_criterion(y, state_values)\n",
    "      # Minimiere Verlust\n",
    "      self.actor_op.zero_grad()\n",
    "      actor_loss.backward()\n",
    "      self.actor_op.step()\n",
    "\n",
    "      if self.t_step % UPDATE_EVERY == 0:\n",
    "        self.soft_update(self.actor_online, self.actor_target)\n",
    "      # return actor_loss.item()\n",
    "\n",
    "\n",
    "  def soft_update(self, local_model, target_model, tau=TAU):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gegebene Code-Implementierung enthält eine Handelsumgebung für eine Krypto-Währung im Verstärkungslernen. Die Klasse `SingleAssetTradingEnvironment` repräsentiert diese Umgebung, in der der Agent mit den Funktionen `reset` und `step` interagiert.\n",
    "\n",
    "Der Konstruktor initialisiert die Handelsumgebung mit verschiedenen Parametern wie historischen Preisdaten, anfänglichem Kapital, Transaktionskostenrate und anderen steuernden Parametern. Die Methode `reset` setzt die Umgebung auf den Anfangszustand zurück, während `step` es dem Agenten ermöglicht, eine Aktion auszuführen und den nächsten Zustand, die Belohnung und den Terminalstatus zu erhalten.\n",
    "\n",
    "Die Handelsumgebung ermöglicht es dem Agenten, Handelsentscheidungen zu treffen und seine Strategie im Laufe der Zeit zu verbessern. Die Umgebung kann an spezifische Handelsszenarien angepasst werden, indem Parameter wie Kapitalanteil, Schwellenwerte und andere Aspekte der Umgebung geändert werden. Insgesamt bietet die Handelsumgebung eine flexible Plattform, um Handelsstrategien im Kontext von Verstärkungslernen zu erforschen und zu entwickeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAssetTradingEnvironment:\n",
    "  \"\"\"\n",
    "  Trading Umgebung für eine Krypto-Währung\n",
    "  Der Agent interagiert mit der Umgebungsklasse über die Funktion step().\n",
    "  Aktionsraum: {-1: Verkaufen, 0: Nichts tun, 1: Kaufen}\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, asset_data,\n",
    "               initial_money=CAPITAL, trans_cost=COST, store_flag=1, asset_ph=0, \n",
    "               capital_frac=0.2, running_thresh=0.1, cap_thresh=0.3):\n",
    "\n",
    "    self.past_holding = asset_ph\n",
    "    self.capital_frac = capital_frac # Fraction of capital to invest each time.\n",
    "    self.cap_thresh = cap_thresh\n",
    "    self.running_thresh = running_thresh\n",
    "    self.trans_cost = trans_cost\n",
    "\n",
    "    self.asset_data = asset_data\n",
    "    self.terminal_idx = len(self.asset_data) - 1\n",
    "    self.scaler = self.asset_data.scaler    \n",
    "\n",
    "    self.initial_cap = initial_money\n",
    "\n",
    "    self.capital = self.initial_cap\n",
    "    self.running_capital = self.capital\n",
    "    self.asset_inv = self.past_holding\n",
    "\n",
    "    self.pointer = 0\n",
    "    self.next_return, self.current_state = 0, None\n",
    "    self.prev_act = 0\n",
    "    self.current_act = 0\n",
    "    self.current_reward = 0\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.done = False\n",
    "\n",
    "    self.store_flag = store_flag\n",
    "    if self.store_flag == 1:\n",
    "      self.store = {\"action_store\": [],\n",
    "                    \"reward_store\": [],\n",
    "                    \"running_capital\": [],\n",
    "                    \"port_ret\": []}\n",
    "\n",
    "\n",
    "  def reset(self):\n",
    "    self.capital = self.initial_cap\n",
    "    self.running_capital = self.capital\n",
    "    self.asset_inv = self.past_holding\n",
    "\n",
    "    self.pointer = 0\n",
    "    self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "    self.prev_act = 0\n",
    "    self.current_act = 0\n",
    "    self.current_reward = 0\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.done = False\n",
    "    \n",
    "    if self.store_flag == 1:\n",
    "      self.store = {\"action_store\": [],\n",
    "                    \"reward_store\": [],\n",
    "                    \"running_capital\": [],\n",
    "                    \"port_ret\": []}\n",
    "\n",
    "    return self.current_state\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    self.current_act = action\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.current_reward = self.calculate_reward()\n",
    "    self.prev_act = self.current_act\n",
    "    self.pointer += 1\n",
    "    self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "    self.done = self.check_terminal()\n",
    "\n",
    "    if self.done:\n",
    "      reward_offset = 0\n",
    "      ret = (self.store['running_capital'][-1]/self.store['running_capital'][-0]) - 1\n",
    "      if self.pointer < self.terminal_idx:\n",
    "        reward_offset += -1 * max(0.5, 1 - self.pointer/self.terminal_idx)\n",
    "      if self.store_flag:\n",
    "        reward_offset += 10 * ret\n",
    "      self.current_reward += reward_offset\n",
    "\n",
    "    if self.store_flag:\n",
    "      self.store[\"action_store\"].append(self.current_act)\n",
    "      self.store[\"reward_store\"].append(self.current_reward)\n",
    "      self.store[\"running_capital\"].append(self.capital)\n",
    "      info = self.store\n",
    "    else:\n",
    "      info = None\n",
    "    \n",
    "    return self.current_state, self.current_reward, self.done, info\n",
    "\n",
    "\n",
    "  def calculate_reward(self):\n",
    "    investment = self.running_capital * self.capital_frac\n",
    "    reward_offset = 0\n",
    "\n",
    "    # Kaufen\n",
    "    if self.current_act == 1: \n",
    "      if self.running_capital > self.initial_cap * self.running_thresh:\n",
    "        self.running_capital -= investment\n",
    "        asset_units = investment/self.current_price\n",
    "        self.asset_inv += asset_units\n",
    "        self.current_price *= (1 - self.trans_cost)\n",
    "\n",
    "    # Verkaufen\n",
    "    elif self.current_act == -1:\n",
    "      if self.asset_inv > 0:\n",
    "        self.running_capital += self.asset_inv * self.current_price * (1 - self.trans_cost)\n",
    "        self.asset_inv = 0\n",
    "\n",
    "    # Nichts machen\n",
    "    elif self.current_act == 0:\n",
    "      if self.prev_act == 0:\n",
    "        reward_offset += -0.1\n",
    "      pass\n",
    "    \n",
    "    # Belohnen\n",
    "    prev_cap = self.capital\n",
    "    self.capital = self.running_capital + (self.asset_inv) * self.current_price\n",
    "    reward = 100*(self.next_return) * self.current_act - np.abs(self.current_act - self.prev_act) * self.trans_cost\n",
    "    if self.store_flag==1:\n",
    "      self.store['port_ret'].append((self.capital - prev_cap)/prev_cap)\n",
    "    \n",
    "    if reward < 0:\n",
    "      reward *= NEG_MUL  # Agent risikobereiter machen\n",
    "    reward += reward_offset\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "  def check_terminal(self):\n",
    "    if self.pointer == self.terminal_idx:\n",
    "      return True\n",
    "    elif self.capital <= self.initial_cap * self.cap_thresh:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "\n",
    "  def get_state(self, idx):\n",
    "    state = self.asset_data[idx][1:]\n",
    "    state = self.scaler.transform(state.reshape(1, -1))\n",
    "\n",
    "    state = np.concatenate([state, [[self.capital/self.initial_cap,\n",
    "                                     self.running_capital/self.capital,\n",
    "                                     self.asset_inv * self.current_price/self.initial_cap,\n",
    "                                     self.prev_act]]], axis=-1)\n",
    "    \n",
    "    next_ret = self.asset_data[idx][0]\n",
    "    return next_ret, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code initialisiert eine Handelsumgebung für verschiedene Kryptowährungen und trainiert einen Agenten mit dem DQNAgent-Modell mittels Verstärkungslernen. Die Hauptschleife führt das Training über mehrere Episoden durch, während der Agent in jeder Episode mit den Umgebungen interagiert und Handelsentscheidungen trifft. Die Leistung des Agenten wird durch kumulierte Belohnungen und Kapitalveränderungen während des Handels bewertet. Der Agent speichert die besten Gewichte während der Validierung, um die besten Ergebnisse zu behalten. Insgesamt wird eine Verstärkungslernen-basierte Handelsstrategie entwickelt, um den Agenten profitabel in verschiedenen Kryptowährungen handeln zu lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Episode: 1, Train Score: -14167.51105, Validation Score: -6109.36912\n",
      "Episode: 1, Train Value: $1897034.06189, Validation Value: $243896.47630 \n",
      "\n",
      "Episode: 2, Train Score: -12512.99836, Validation Score: -6318.94622\n",
      "Episode: 2, Train Value: $1749389.80608, Validation Value: $165444.77750 \n",
      "\n",
      "Episode: 3, Train Score: -7893.07951, Validation Score: -4981.11634\n",
      "Episode: 3, Train Value: $3614299.94485, Validation Value: $248178.94405 \n",
      "\n",
      "Episode: 4, Train Score: -4756.44993, Validation Score: -4571.22406\n",
      "Episode: 4, Train Value: $7731817.00308, Validation Value: $170737.97595 \n",
      "\n",
      "Episode: 5, Train Score: -1837.10979, Validation Score: -3852.84098\n",
      "Episode: 5, Train Value: $11018881.55393, Validation Value: $204190.81237 \n",
      "\n",
      "Episode: 6, Train Score: 58.34035, Validation Score: -3686.67907\n",
      "Episode: 6, Train Value: $17123530.17562, Validation Value: $238482.76213 \n",
      "\n",
      "Episode: 7, Train Score: 1529.23181, Validation Score: -2905.75153\n",
      "Episode: 7, Train Value: $17311515.32116, Validation Value: $354678.38578 \n",
      "\n",
      "Episode: 8, Train Score: 2746.11116, Validation Score: -3567.65864\n",
      "Episode: 8, Train Value: $21400094.88770, Validation Value: $237782.27874 \n",
      "\n",
      "Episode: 9, Train Score: 6116.11454, Validation Score: -3451.72658\n",
      "Episode: 9, Train Value: $40756767.90057, Validation Value: $211748.98732 \n",
      "\n",
      "Episode: 10, Train Score: 6019.27490, Validation Score: -4045.22380\n",
      "Episode: 10, Train Value: $33918656.17394, Validation Value: $241916.59525 \n",
      "\n",
      "Episode: 11, Train Score: 9072.00070, Validation Score: -3875.67633\n",
      "Episode: 11, Train Value: $57726976.08396, Validation Value: $298253.70550 \n",
      "\n",
      "Episode: 12, Train Score: 11948.41209, Validation Score: -4267.52263\n",
      "Episode: 12, Train Value: $69068878.31423, Validation Value: $149879.81633 \n",
      "\n",
      "Episode: 13, Train Score: 11721.61573, Validation Score: -3953.25889\n",
      "Episode: 13, Train Value: $53697312.89572, Validation Value: $175611.07623 \n",
      "\n",
      "Episode: 14, Train Score: 13804.77534, Validation Score: -3976.21074\n",
      "Episode: 14, Train Value: $88470273.00637, Validation Value: $139188.41540 \n",
      "\n",
      "Episode: 15, Train Score: 16876.61450, Validation Score: -3759.13176\n",
      "Episode: 15, Train Value: $119329857.86390, Validation Value: $157821.27875 \n",
      "\n",
      "Episode: 16, Train Score: 34273.15496, Validation Score: -6256.43455\n",
      "Episode: 16, Train Value: $325050232.03873, Validation Value: $134596.56990 \n",
      "\n",
      "Episode: 17, Train Score: 8741.81810, Validation Score: -3173.26632\n",
      "Episode: 17, Train Value: $78178851.54628, Validation Value: $426658.97811 \n",
      "\n",
      "Episode: 18, Train Score: 10642.06540, Validation Score: -4633.77709\n",
      "Episode: 18, Train Value: $69091269.89937, Validation Value: $223757.92498 \n",
      "\n",
      "Episode: 19, Train Score: 12425.05676, Validation Score: -4709.77019\n",
      "Episode: 19, Train Value: $93794867.78467, Validation Value: $212618.47552 \n",
      "\n",
      "Episode: 20, Train Score: 22394.08559, Validation Score: -4133.39338\n",
      "Episode: 20, Train Value: $169324886.90138, Validation Value: $111356.14299 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Umgebung und Agenteninitiierung\n",
    "\n",
    "## Kryptowährungen\n",
    "asset_codes = [\"ETH-USD\", \"BNB-USD\", \"XRP-USD\", \"SOL-USD\", \"DOGE-USD\", \n",
    "               \"ADA-USD\", \"MATIC-USD\", \"AVAX-USD\", \"WAVES-USD\"]\n",
    "\n",
    "## Trainings- und Testumgebungen\n",
    "assets = [DataGetter(a, start_date=\"2015-01-01\", end_date=\"2021-05-01\") for a in asset_codes]\n",
    "test_assets = [DataGetter(a, start_date=\"2021-05-01\", end_date=\"2022-05-01\", freq=\"1d\") for a in asset_codes]\n",
    "envs = [SingleAssetTradingEnvironment(a) for a in assets]\n",
    "test_envs = [SingleAssetTradingEnvironment(a) for a in test_assets]\n",
    "\n",
    "## Agent\n",
    "memory = ReplayMemory()\n",
    "agent = DQNAgent(actor_net=DuellingDQN, memory=memory)\n",
    "\n",
    "\n",
    "# Hauptschleife\n",
    "N_EPISODES = 20 # Anzahl an Episoden/Epochen\n",
    "scores = []\n",
    "eps = EPS_START\n",
    "act_dict = {0:-1, 1:1, 2:0}\n",
    "\n",
    "te_score_min = -np.Inf\n",
    "for episode in range(1, 1 + N_EPISODES):\n",
    "  counter = 0\n",
    "  episode_score = 0\n",
    "  episode_score2 = 0\n",
    "  test_score = 0\n",
    "  test_score2 = 0\n",
    "\n",
    "  for env in envs:\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = state.reshape(-1, STATE_SPACE)\n",
    "    while True:\n",
    "      actions = agent.act(state, eps)\n",
    "      action = act_dict[actions]\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      next_state = next_state.reshape(-1, STATE_SPACE)\n",
    "\n",
    "      t = Transition(state, actions, reward, next_state, done)\n",
    "      agent.memory.store(t)\n",
    "      agent.learn()\n",
    "\n",
    "      state = next_state\n",
    "      score += reward\n",
    "      counter += 1\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    episode_score += score\n",
    "    episode_score2 += (env.store['running_capital'][-1] - env.store['running_capital'][0])\n",
    "\n",
    "  scores.append(episode_score)\n",
    "  eps = max(EPS_END, EPS_DECAY * eps)\n",
    "\n",
    "  for i, test_env in enumerate(test_envs):\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    score_te = 0\n",
    "    scores_te = [score_te]\n",
    "\n",
    "    while True:\n",
    "      actions = agent.act(state)\n",
    "      action = act_dict[actions]\n",
    "      next_state, reward, done, _ = test_env.step(action)\n",
    "      next_state = next_state.reshape(-1, STATE_SPACE)\n",
    "      state= next_state\n",
    "      score_te += reward\n",
    "      scores_te.append(score_te)\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    test_score += score_te\n",
    "    test_score2 += (test_env.store['running_capital'][-1] - test_env.store['running_capital'][0])\n",
    "  if test_score > te_score_min:\n",
    "    te_score_min = test_score\n",
    "    torch.save(agent.actor_online.state_dict(), \"online.pt\")\n",
    "    torch.save(agent.actor_target.state_dict(), \"target.pt\")\n",
    "\n",
    "  print(f\"Episode: {episode}, Train Score: {episode_score:.5f}, Validation Score: {test_score:.5f}\")\n",
    "  print(f\"Episode: {episode}, Train Value: ${episode_score2:.5f}, Validation Value: ${test_score2:.5f}\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
