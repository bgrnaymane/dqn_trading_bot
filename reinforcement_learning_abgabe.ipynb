{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import nntplib as nn\n",
    "from typing import Deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from parameters_global import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobalParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = 34  #geÃ¤ndert anstatt 28\n",
    "ACTION_SPACE = 3\n",
    "\n",
    "ACTION_LOW = -1\n",
    "ACTION_HIGH = 1\n",
    "\n",
    "GAMMA = 0.9995\n",
    "TAU = 1e-3\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 0.9\n",
    "\n",
    "MEMORY_LEN = 10000\n",
    "MEMORY_THRESH = 500\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "LR_DQN = 5e-4\n",
    "\n",
    "LEARN_AFTER = MEMORY_THRESH\n",
    "LEARN_EVERY = 3\n",
    "UPDATE_EVERY = 9\n",
    "\n",
    "COST = 3e-4\n",
    "CAPITAL = 100000\n",
    "NEG_MUL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "  \"\"\"\n",
    "  The class for getting data for assets.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, asset=\"BTC-USD\", start_date=None, end_date=None, freq=\"1d\", \n",
    "               timeframes=[1, 2, 5, 10, 20, 40]):\n",
    "    self.asset = asset\n",
    "    self.sd = start_date\n",
    "    self.ed = end_date\n",
    "    self.freq = freq\n",
    "\n",
    "    self.timeframes = timeframes\n",
    "    self.getData()\n",
    "\n",
    "    self.scaler = StandardScaler()\n",
    "    self.scaler.fit(self.data[:, 1:])\n",
    "\n",
    "\n",
    "  def getData(self):\n",
    "    \n",
    "    asset = self.asset  \n",
    "    if self.sd is not None and self.ed is not None:\n",
    "      df =  yf.download([asset], start=self.sd, end=self.ed, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], start=self.sd, end=self.ed, interval=self.freq)\n",
    "    elif self.sd is None and self.ed is not None:\n",
    "      df =  yf.download([asset], end=self.ed, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], end=self.ed, interval=self.freq)\n",
    "    elif self.sd is not None and self.ed is None:\n",
    "      df =  yf.download([asset], start=self.sd, interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], start=self.sd, interval=self.freq)\n",
    "    else:\n",
    "      df = yf.download([asset], period=\"max\", interval=self.freq)\n",
    "      df_spy = yf.download([\"BTC-USD\"], interval=self.freq)\n",
    "    \n",
    "    # Reward - Not included in Observation Space.\n",
    "    df[\"rf\"] = df[\"Adj Close\"].pct_change().shift(-1)\n",
    "\n",
    "    # Returns and Trading Volume Changes\n",
    "    for i in self.timeframes:\n",
    "      df_spy[f\"spy_ret-{i}\"] = df_spy[\"Adj Close\"].pct_change(i)\n",
    "      df_spy[f\"spy_v-{i}\"] = df_spy[\"Volume\"].pct_change(i)\n",
    "\n",
    "      df[f\"r-{i}\"] = df[\"Adj Close\"].pct_change(i)      \n",
    "      df[f\"v-{i}\"] = df[\"Volume\"].pct_change(i)\n",
    "    \n",
    "    # Volatility\n",
    "    for i in [5, 10, 20, 40]:\n",
    "      df[f'sig-{i}'] = np.log(1 + df[\"r-1\"]).rolling(i).std()\n",
    "\n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    df[\"macd_lmw\"] = df[\"r-1\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd_smw\"] = df[\"r-1\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"macd_bl\"] = df[\"r-1\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"macd\"] = df[\"macd_smw\"] - df[\"macd_lmw\"]\n",
    "\n",
    "    # Relative Strength Indicator (RSI)\n",
    "    rsi_lb = 5\n",
    "    pos_gain = df[\"r-1\"].where(df[\"r-1\"] > 0, 0).ewm(rsi_lb).mean()\n",
    "    neg_gain = df[\"r-1\"].where(df[\"r-1\"] < 0, 0).ewm(rsi_lb).mean()\n",
    "    rs = np.abs(pos_gain/neg_gain)\n",
    "    df[\"rsi\"] = 100 * rs/(1 + rs)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bollinger_lback = 10\n",
    "    df[\"bollinger\"] = df[\"r-1\"].ewm(bollinger_lback).mean()\n",
    "    df[\"low_bollinger\"] = df[\"bollinger\"] - 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "    df[\"high_bollinger\"] = df[\"bollinger\"] + 2 * df[\"r-1\"].rolling(bollinger_lback).std()\n",
    "    # print(df.columns)\n",
    "    # print(df_spy.columns)\n",
    "    # SP500\n",
    "    # df = df.merge(df_spy[[f\"spy_ret-{i}\" for i in self.timeframes] + [f\"spy_sig-{i}\" for i in [5, 10, 20, 40]]], \n",
    "    #               how=\"left\", right_index=True, left_index=True)\n",
    "    df = df.merge(df_spy[[f\"spy_ret-{i}\" for i in self.timeframes]], \n",
    "              how=\"left\", right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "    # Filtering\n",
    "    for c in df.columns:\n",
    "      df[c].interpolate('linear', limit_direction='both', inplace=True)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    self.frame = df\n",
    "    self.data = np.array(df.iloc[:, 6:])\n",
    "    return\n",
    "\n",
    "\n",
    "  def scaleData(self):\n",
    "    self.scaled_data = self.scaler.fit_transform(self.data[:, 1:])\n",
    "    return\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx, col_idx=None):\n",
    "    if col_idx is None:\n",
    "      return self.data[idx]\n",
    "    elif col_idx < len(list(self.data.columns)):\n",
    "      return self.data[idx][col_idx]\n",
    "    else:\n",
    "      raise IndexError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"States\", \"Actions\", \"Rewards\", \"NextStates\", \"Dones\"])\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "  \"\"\"\n",
    "  Implementation of Agent memory\n",
    "  \"\"\"\n",
    "  def __init__(self, capacity=MEMORY_LEN):\n",
    "    self.memory = Deque(maxlen=capacity)\n",
    "\n",
    "  def store(self, t):\n",
    "    self.memory.append(t)\n",
    "\n",
    "  def sample(self, n):\n",
    "    a = random.sample(self.memory, n)\n",
    "    return a\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuellingDQN(nn.Module):\n",
    "  \"\"\"\n",
    "  Acrchitecture for Duelling Deep Q Network Agent\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_dim=STATE_SPACE, output_dim=ACTION_SPACE):\n",
    "    super(DuellingDQN, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "    self.fc1 = nn.Linear(self.input_dim, 500)\n",
    "    self.fc2 = nn.Linear(500, 500)\n",
    "    self.fc3 = nn.Linear(500, 300)\n",
    "    self.fc4 = nn.Linear(300, 200)\n",
    "    self.fc5 = nn.Linear(200, 10)\n",
    "\n",
    "    self.fcs = nn.Linear(10, 1)\n",
    "    self.fcp = nn.Linear(10, self.output_dim)\n",
    "    self.fco = nn.Linear(self.output_dim + 1, self.output_dim)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.sig = nn.Sigmoid()\n",
    "    self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc1(state))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    x = self.relu(self.fc3(x))\n",
    "    x = self.relu(self.fc4(x))\n",
    "    x = self.relu(self.fc5(x))\n",
    "    xs = self.relu(self.fcs(x))\n",
    "    xp = self.relu(self.fcp(x))\n",
    "\n",
    "    x = xs + xp - xp.mean()\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "  \"\"\"\n",
    "  Implements the Agent components\n",
    "  \"\"\"\n",
    "  DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "  def __init__(self, actor_net=DuellingDQN, memory=ReplayMemory()):\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.actor_online = actor_net(STATE_SPACE, ACTION_SPACE).to(DEVICE)\n",
    "    self.actor_target = actor_net(STATE_SPACE, ACTION_SPACE).to(DEVICE)\n",
    "    self.actor_target.load_state_dict(self.actor_online.state_dict())\n",
    "    self.actor_target.eval()\n",
    "\n",
    "    self.memory = memory\n",
    "\n",
    "    self.actor_criterion = nn.MSELoss()\n",
    "    self.actor_op = optim.Adam(self.actor_online.parameters(), lr=LR_DQN)\n",
    "\n",
    "    self.t_step = 0\n",
    "\n",
    "\n",
    "  def act(self, state, eps=0.):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    self.t_step += 1\n",
    "    state = torch.from_numpy(state).float().to(DEVICE).view(1, -1)\n",
    "    \n",
    "    self.actor_online.eval()\n",
    "    with torch.no_grad():\n",
    "      actions = self.actor_online(state)\n",
    "    self.actor_online.train()\n",
    "\n",
    "    if random.random() > eps:\n",
    "      act = np.argmax(actions.cpu().data.numpy())\n",
    "    else:\n",
    "      act = random.choice(np.arange(ACTION_SPACE))\n",
    "    return int(act)\n",
    "\n",
    "\n",
    "  def learn(self):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if len(self.memory) <= MEMORY_THRESH:\n",
    "      return 0\n",
    "\n",
    "    if self.t_step > LEARN_AFTER and self.t_step % LEARN_EVERY==0:\n",
    "    # Sample experiences from the Memory\n",
    "      batch = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "      states = np.vstack([t.States for t in batch])\n",
    "      states = torch.from_numpy(states).float().to(DEVICE)\n",
    "\n",
    "      actions = np.vstack([t.Actions for t in batch])\n",
    "      actions = torch.from_numpy(actions).float().to(DEVICE)\n",
    "\n",
    "      rewards = np.vstack([t.Rewards for t in batch])\n",
    "      rewards = torch.from_numpy(rewards).float().to(DEVICE)\n",
    "\n",
    "      next_states = np.vstack([t.NextStates for t in batch])\n",
    "      next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "\n",
    "      dones = np.vstack([t.Dones for t in batch]).astype(np.uint8)\n",
    "      dones = torch.from_numpy(dones).float().to(DEVICE)\n",
    "\n",
    "      # ACTOR UPDATE\n",
    "      # Compute next state actions and state values\n",
    "      next_state_values = self.actor_target(next_states).max(1)[0].unsqueeze(1)\n",
    "      y = rewards + (1-dones) * GAMMA * next_state_values\n",
    "      state_values = self.actor_online(states).gather(1, actions.type(torch.int64))\n",
    "      # Compute Actor loss\n",
    "      actor_loss = self.actor_criterion(y, state_values)\n",
    "      # Minimize Actor loss\n",
    "      self.actor_op.zero_grad()\n",
    "      actor_loss.backward()\n",
    "      self.actor_op.step()\n",
    "\n",
    "      if self.t_step % UPDATE_EVERY == 0:\n",
    "        self.soft_update(self.actor_online, self.actor_target)\n",
    "      # return actor_loss.item()\n",
    "\n",
    "\n",
    "  def soft_update(self, local_model, target_model, tau=TAU):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAssetTradingEnvironment:\n",
    "  \"\"\"\n",
    "  Trading Environment for trading a single asset.\n",
    "  The Agent interacts with the environment class through the step() function.\n",
    "  Action Space: {-1: Sell, 0: Do Nothing, 1: Buy}\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, asset_data,\n",
    "               initial_money=CAPITAL, trans_cost=COST, store_flag=1, asset_ph=0, \n",
    "               capital_frac=0.2, running_thresh=0.1, cap_thresh=0.3):\n",
    "\n",
    "    self.past_holding = asset_ph\n",
    "    self.capital_frac = capital_frac # Fraction of capital to invest each time.\n",
    "    self.cap_thresh = cap_thresh\n",
    "    self.running_thresh = running_thresh\n",
    "    self.trans_cost = trans_cost\n",
    "\n",
    "    self.asset_data = asset_data\n",
    "    self.terminal_idx = len(self.asset_data) - 1\n",
    "    self.scaler = self.asset_data.scaler    \n",
    "\n",
    "    self.initial_cap = initial_money\n",
    "\n",
    "    self.capital = self.initial_cap\n",
    "    self.running_capital = self.capital\n",
    "    self.asset_inv = self.past_holding\n",
    "\n",
    "    self.pointer = 0\n",
    "    self.next_return, self.current_state = 0, None\n",
    "    self.prev_act = 0\n",
    "    self.current_act = 0\n",
    "    self.current_reward = 0\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.done = False\n",
    "\n",
    "    self.store_flag = store_flag\n",
    "    if self.store_flag == 1:\n",
    "      self.store = {\"action_store\": [],\n",
    "                    \"reward_store\": [],\n",
    "                    \"running_capital\": [],\n",
    "                    \"port_ret\": []}\n",
    "\n",
    "\n",
    "  def reset(self):\n",
    "    self.capital = self.initial_cap\n",
    "    self.running_capital = self.capital\n",
    "    self.asset_inv = self.past_holding\n",
    "\n",
    "    self.pointer = 0\n",
    "    self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "    self.prev_act = 0\n",
    "    self.current_act = 0\n",
    "    self.current_reward = 0\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.done = False\n",
    "    \n",
    "    if self.store_flag == 1:\n",
    "      self.store = {\"action_store\": [],\n",
    "                    \"reward_store\": [],\n",
    "                    \"running_capital\": [],\n",
    "                    \"port_ret\": []}\n",
    "\n",
    "    return self.current_state\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    self.current_act = action\n",
    "    self.current_price = self.asset_data.frame.iloc[self.pointer, :]['Adj Close']\n",
    "    self.current_reward = self.calculate_reward()\n",
    "    self.prev_act = self.current_act\n",
    "    self.pointer += 1\n",
    "    self.next_return, self.current_state = self.get_state(self.pointer)\n",
    "    self.done = self.check_terminal()\n",
    "\n",
    "    if self.done:\n",
    "      reward_offset = 0\n",
    "      ret = (self.store['running_capital'][-1]/self.store['running_capital'][-0]) - 1\n",
    "      if self.pointer < self.terminal_idx:\n",
    "        reward_offset += -1 * max(0.5, 1 - self.pointer/self.terminal_idx)\n",
    "      if self.store_flag:\n",
    "        reward_offset += 10 * ret\n",
    "      self.current_reward += reward_offset\n",
    "\n",
    "    if self.store_flag:\n",
    "      self.store[\"action_store\"].append(self.current_act)\n",
    "      self.store[\"reward_store\"].append(self.current_reward)\n",
    "      self.store[\"running_capital\"].append(self.capital)\n",
    "      info = self.store\n",
    "    else:\n",
    "      info = None\n",
    "    \n",
    "    return self.current_state, self.current_reward, self.done, info\n",
    "\n",
    "\n",
    "  def calculate_reward(self):\n",
    "    investment = self.running_capital * self.capital_frac\n",
    "    reward_offset = 0\n",
    "\n",
    "    # Buy Action\n",
    "    if self.current_act == 1: \n",
    "      if self.running_capital > self.initial_cap * self.running_thresh:\n",
    "        self.running_capital -= investment\n",
    "        asset_units = investment/self.current_price\n",
    "        self.asset_inv += asset_units\n",
    "        self.current_price *= (1 - self.trans_cost)\n",
    "\n",
    "    # Sell Action\n",
    "    elif self.current_act == -1:\n",
    "      if self.asset_inv > 0:\n",
    "        self.running_capital += self.asset_inv * self.current_price * (1 - self.trans_cost)\n",
    "        self.asset_inv = 0\n",
    "\n",
    "    # Do Nothing\n",
    "    elif self.current_act == 0:\n",
    "      if self.prev_act == 0:\n",
    "        reward_offset += -0.1\n",
    "      pass\n",
    "    \n",
    "    # Reward to give\n",
    "    prev_cap = self.capital\n",
    "    self.capital = self.running_capital + (self.asset_inv) * self.current_price\n",
    "    reward = 100*(self.next_return) * self.current_act - np.abs(self.current_act - self.prev_act) * self.trans_cost\n",
    "    if self.store_flag==1:\n",
    "      self.store['port_ret'].append((self.capital - prev_cap)/prev_cap)\n",
    "    \n",
    "    if reward < 0:\n",
    "      reward *= NEG_MUL  # To make the Agent more risk averse towards negative returns.\n",
    "    reward += reward_offset\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "  def check_terminal(self):\n",
    "    if self.pointer == self.terminal_idx:\n",
    "      return True\n",
    "    elif self.capital <= self.initial_cap * self.cap_thresh:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "\n",
    "  def get_state(self, idx):\n",
    "    state = self.asset_data[idx][1:]\n",
    "    state = self.scaler.transform(state.reshape(1, -1))\n",
    "\n",
    "    state = np.concatenate([state, [[self.capital/self.initial_cap,\n",
    "                                     self.running_capital/self.capital,\n",
    "                                     self.asset_inv * self.current_price/self.initial_cap,\n",
    "                                     self.prev_act]]], axis=-1)\n",
    "    \n",
    "    next_ret = self.asset_data[idx][0]\n",
    "    return next_ret, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Episode: 1, Train Score: -14167.51105, Validation Score: -6109.36912\n",
      "Episode: 1, Train Value: $1897034.06189, Validation Value: $243896.47630 \n",
      "\n",
      "Episode: 2, Train Score: -12512.99836, Validation Score: -6318.94622\n",
      "Episode: 2, Train Value: $1749389.80608, Validation Value: $165444.77750 \n",
      "\n",
      "Episode: 3, Train Score: -7893.07951, Validation Score: -4981.11634\n",
      "Episode: 3, Train Value: $3614299.94485, Validation Value: $248178.94405 \n",
      "\n",
      "Episode: 4, Train Score: -4756.44993, Validation Score: -4571.22406\n",
      "Episode: 4, Train Value: $7731817.00308, Validation Value: $170737.97595 \n",
      "\n",
      "Episode: 5, Train Score: -1837.10979, Validation Score: -3852.84098\n",
      "Episode: 5, Train Value: $11018881.55393, Validation Value: $204190.81237 \n",
      "\n",
      "Episode: 6, Train Score: 58.34035, Validation Score: -3686.67907\n",
      "Episode: 6, Train Value: $17123530.17562, Validation Value: $238482.76213 \n",
      "\n",
      "Episode: 7, Train Score: 1529.23181, Validation Score: -2905.75153\n",
      "Episode: 7, Train Value: $17311515.32116, Validation Value: $354678.38578 \n",
      "\n",
      "Episode: 8, Train Score: 2746.11116, Validation Score: -3567.65864\n",
      "Episode: 8, Train Value: $21400094.88770, Validation Value: $237782.27874 \n",
      "\n",
      "Episode: 9, Train Score: 6116.11454, Validation Score: -3451.72658\n",
      "Episode: 9, Train Value: $40756767.90057, Validation Value: $211748.98732 \n",
      "\n",
      "Episode: 10, Train Score: 6019.27490, Validation Score: -4045.22380\n",
      "Episode: 10, Train Value: $33918656.17394, Validation Value: $241916.59525 \n",
      "\n",
      "Episode: 11, Train Score: 9072.00070, Validation Score: -3875.67633\n",
      "Episode: 11, Train Value: $57726976.08396, Validation Value: $298253.70550 \n",
      "\n",
      "Episode: 12, Train Score: 11948.41209, Validation Score: -4267.52263\n",
      "Episode: 12, Train Value: $69068878.31423, Validation Value: $149879.81633 \n",
      "\n",
      "Episode: 13, Train Score: 11721.61573, Validation Score: -3953.25889\n",
      "Episode: 13, Train Value: $53697312.89572, Validation Value: $175611.07623 \n",
      "\n",
      "Episode: 14, Train Score: 13804.77534, Validation Score: -3976.21074\n",
      "Episode: 14, Train Value: $88470273.00637, Validation Value: $139188.41540 \n",
      "\n",
      "Episode: 15, Train Score: 16876.61450, Validation Score: -3759.13176\n",
      "Episode: 15, Train Value: $119329857.86390, Validation Value: $157821.27875 \n",
      "\n",
      "Episode: 16, Train Score: 34273.15496, Validation Score: -6256.43455\n",
      "Episode: 16, Train Value: $325050232.03873, Validation Value: $134596.56990 \n",
      "\n",
      "Episode: 17, Train Score: 8741.81810, Validation Score: -3173.26632\n",
      "Episode: 17, Train Value: $78178851.54628, Validation Value: $426658.97811 \n",
      "\n",
      "Episode: 18, Train Score: 10642.06540, Validation Score: -4633.77709\n",
      "Episode: 18, Train Value: $69091269.89937, Validation Value: $223757.92498 \n",
      "\n",
      "Episode: 19, Train Score: 12425.05676, Validation Score: -4709.77019\n",
      "Episode: 19, Train Value: $93794867.78467, Validation Value: $212618.47552 \n",
      "\n",
      "Episode: 20, Train Score: 22394.08559, Validation Score: -4133.39338\n",
      "Episode: 20, Train Value: $169324886.90138, Validation Value: $111356.14299 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment and Agent Initiation\n",
    "\n",
    "## Cryptocurrency Tickers\n",
    "asset_codes = [\"ETH-USD\", \"BNB-USD\", \"XRP-USD\", \"SOL-USD\", \"DOGE-USD\", \n",
    "               \"ADA-USD\", \"MATIC-USD\", \"AVAX-USD\", \"WAVES-USD\"]\n",
    "\n",
    "## Training and Testing Environments\n",
    "assets = [DataGetter(a, start_date=\"2015-01-01\", end_date=\"2021-05-01\") for a in asset_codes]\n",
    "test_assets = [DataGetter(a, start_date=\"2021-05-01\", end_date=\"2022-05-01\", freq=\"1d\") for a in asset_codes]\n",
    "envs = [SingleAssetTradingEnvironment(a) for a in assets]\n",
    "test_envs = [SingleAssetTradingEnvironment(a) for a in test_assets]\n",
    "\n",
    "## Agent\n",
    "memory = ReplayMemory()\n",
    "agent = DQNAgent(actor_net=DuellingDQN, memory=memory)\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "N_EPISODES = 20 # No of episodes/epochs\n",
    "scores = []\n",
    "eps = EPS_START\n",
    "act_dict = {0:-1, 1:1, 2:0}\n",
    "\n",
    "te_score_min = -np.Inf\n",
    "for episode in range(1, 1 + N_EPISODES):\n",
    "  counter = 0\n",
    "  episode_score = 0\n",
    "  episode_score2 = 0\n",
    "  test_score = 0\n",
    "  test_score2 = 0\n",
    "\n",
    "  for env in envs:\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = state.reshape(-1, STATE_SPACE)\n",
    "    while True:\n",
    "      actions = agent.act(state, eps)\n",
    "      action = act_dict[actions]\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      next_state = next_state.reshape(-1, STATE_SPACE)\n",
    "\n",
    "      t = Transition(state, actions, reward, next_state, done)\n",
    "      agent.memory.store(t)\n",
    "      agent.learn()\n",
    "\n",
    "      state = next_state\n",
    "      score += reward\n",
    "      counter += 1\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    episode_score += score\n",
    "    episode_score2 += (env.store['running_capital'][-1] - env.store['running_capital'][0])\n",
    "\n",
    "  scores.append(episode_score)\n",
    "  eps = max(EPS_END, EPS_DECAY * eps)\n",
    "\n",
    "  for i, test_env in enumerate(test_envs):\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    score_te = 0\n",
    "    scores_te = [score_te]\n",
    "\n",
    "    while True:\n",
    "      actions = agent.act(state)\n",
    "      action = act_dict[actions]\n",
    "      next_state, reward, done, _ = test_env.step(action)\n",
    "      next_state = next_state.reshape(-1, STATE_SPACE)\n",
    "      state= next_state\n",
    "      score_te += reward\n",
    "      scores_te.append(score_te)\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    test_score += score_te\n",
    "    test_score2 += (test_env.store['running_capital'][-1] - test_env.store['running_capital'][0])\n",
    "  if test_score > te_score_min:\n",
    "    te_score_min = test_score\n",
    "    torch.save(agent.actor_online.state_dict(), \"online.pt\")\n",
    "    torch.save(agent.actor_target.state_dict(), \"target.pt\")\n",
    "\n",
    "  print(f\"Episode: {episode}, Train Score: {episode_score:.5f}, Validation Score: {test_score:.5f}\")\n",
    "  print(f\"Episode: {episode}, Train Value: ${episode_score2:.5f}, Validation Value: ${test_score2:.5f}\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
